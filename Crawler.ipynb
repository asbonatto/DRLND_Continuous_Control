{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous Control\n",
    "\n",
    "---\n",
    "\n",
    "Congratulations for completing the second project of the [Deep Reinforcement Learning Nanodegree](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893) program!  In this notebook, you will learn how to control an agent in a more challenging environment, where the goal is to train a creature with four arms to walk forward.  **Note that this exercise is optional!**\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "We begin by importing the necessary packages.  If the code cell below returns an error, please revisit the project instructions to double-check that you have installed [Unity ML-Agents](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md) and [NumPy](http://www.numpy.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will start the environment!  **_Before running the code cell below_**, change the `file_name` parameter to match the location of the Unity environment that you downloaded.\n",
    "\n",
    "- **Mac**: `\"path/to/Crawler.app\"`\n",
    "- **Windows** (x86): `\"path/to/Crawler_Windows_x86/Crawler.exe\"`\n",
    "- **Windows** (x86_64): `\"path/to/Crawler_Windows_x86_64/Crawler.exe\"`\n",
    "- **Linux** (x86): `\"path/to/Crawler_Linux/Crawler.x86\"`\n",
    "- **Linux** (x86_64): `\"path/to/Crawler_Linux/Crawler.x86_64\"`\n",
    "- **Linux** (x86, headless): `\"path/to/Crawler_Linux_NoVis/Crawler.x86\"`\n",
    "- **Linux** (x86_64, headless): `\"path/to/Crawler_Linux_NoVis/Crawler.x86_64\"`\n",
    "\n",
    "For instance, if you are using a Mac, then you downloaded `Crawler.app`.  If this file is in the same folder as the notebook, then the line below should appear as follows:\n",
    "```\n",
    "env = UnityEnvironment(file_name=\"Crawler.app\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = UnityEnvironment(file_name='envs/Crawler_Windows/Crawler.exe')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Examine the State and Action Spaces\n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents\n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Take Random Actions in the Environment\n",
    "\n",
    "In the next code cell, you will learn how to use the Python API to control the agent and receive feedback from the environment.\n",
    "\n",
    "Once this cell is executed, you will watch the agent's performance, if it selects an action at random with each time step.  A window should pop up that allows you to observe the agent, as it moves through the environment.  \n",
    "\n",
    "Of course, as part of the project, you'll have to change the code so that the agent is able to use its experience to gradually choose better actions when interacting with the environment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_info = env.reset(train_mode=False)[brain_name]     # reset the environment    \n",
    "states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "while True:\n",
    "    actions = np.random.randn(num_agents, action_size) # select an action (for each agent)\n",
    "    actions = np.clip(actions, -1, 1)                  # all actions between -1 and 1\n",
    "    env_info = env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "    next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "    rewards = env_info.rewards                         # get reward (for each agent)\n",
    "    dones = env_info.local_done                        # see if episode finished\n",
    "    scores += env_info.rewards                         # update the score (for each agent)\n",
    "    states = next_states                               # roll over states to next time step\n",
    "    if np.any(dones):                                  # exit loop if episode finished\n",
    "        break\n",
    "print('Total score (averaged over agents) this episode: {}'.format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When finished, you can close the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. It's Your Turn!\n",
    "\n",
    "Now it's your turn to train your own agent to solve the environment!  When training the environment, set `train_mode=True`, so that the line for resetting the environment looks like the following:\n",
    "```python\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: CrawlerBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 129\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 20\n",
      "        Vector Action descriptions: , , , , , , , , , , , , , , , , , , , \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of actions: 20\n",
      "Number of agents: 12\n",
      "States look like: [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  2.25000000e+00\n",
      "  1.00000000e+00  0.00000000e+00  1.78813934e-07  0.00000000e+00\n",
      "  1.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  6.06093168e-01 -1.42857209e-01 -6.06078804e-01  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  1.33339906e+00 -1.42857209e-01\n",
      " -1.33341408e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      " -6.06093347e-01 -1.42857209e-01 -6.06078625e-01  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00 -1.33339953e+00 -1.42857209e-01\n",
      " -1.33341372e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      " -6.06093168e-01 -1.42857209e-01  6.06078804e-01  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00 -1.33339906e+00 -1.42857209e-01\n",
      "  1.33341408e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  6.06093347e-01 -1.42857209e-01  6.06078625e-01  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  1.33339953e+00 -1.42857209e-01\n",
      "  1.33341372e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00]\n",
      "State space dimension: 129\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files\\python36\\lib\\site-packages\\torch\\nn\\functional.py:995: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode   0 of 128 Avg/Last/Min/Max Score : 6.94/6.94/6.94/6.94.\n",
      "Episode   1 of 128 Avg/Last/Min/Max Score : 4.96/2.98/2.98/6.94.\n",
      "Episode   2 of 128 Avg/Last/Min/Max Score : 5.37/6.20/2.98/6.94.\n",
      "Episode   3 of 128 Avg/Last/Min/Max Score : 4.81/3.11/2.98/6.94.\n",
      "Episode   4 of 128 Avg/Last/Min/Max Score : 5.45/8.02/2.98/8.02.\n",
      "Episode   5 of 128 Avg/Last/Min/Max Score : 5.87/7.97/2.98/8.02.\n",
      "Episode   6 of 128 Avg/Last/Min/Max Score : 6.20/8.16/2.98/8.16.\n",
      "Episode   7 of 128 Avg/Last/Min/Max Score : 6.28/6.83/2.98/8.16.\n",
      "Episode   8 of 128 Avg/Last/Min/Max Score : 6.38/7.21/2.98/8.16.\n",
      "Episode   9 of 128 Avg/Last/Min/Max Score : 6.65/9.12/2.98/9.12.\n",
      "Episode  10 of 128 Avg/Last/Min/Max Score : 6.92/9.58/2.98/9.58.\n",
      "Episode  11 of 128 Avg/Last/Min/Max Score : 7.00/7.94/2.98/9.58.\n",
      "Episode  12 of 128 Avg/Last/Min/Max Score : 7.09/8.18/2.98/9.58.\n",
      "Episode  13 of 128 Avg/Last/Min/Max Score : 7.20/8.52/2.98/9.58.\n",
      "Episode  14 of 128 Avg/Last/Min/Max Score : 7.25/7.98/2.98/9.58.\n",
      "Episode  15 of 128 Avg/Last/Min/Max Score : 7.41/9.87/2.98/9.87.\n",
      "Episode  16 of 128 Avg/Last/Min/Max Score : 7.32/5.84/2.98/9.87.\n",
      "Episode  17 of 128 Avg/Last/Min/Max Score : 7.38/8.37/2.98/9.87.\n",
      "Episode  18 of 128 Avg/Last/Min/Max Score : 7.45/8.67/2.98/9.87.\n",
      "Episode  19 of 128 Avg/Last/Min/Max Score : 7.49/8.33/2.98/9.87.\n",
      "Episode  20 of 128 Avg/Last/Min/Max Score : 7.65/10.88/2.98/10.88.\n",
      "Episode  21 of 128 Avg/Last/Min/Max Score : 7.73/9.39/2.98/10.88.\n",
      "Episode  22 of 128 Avg/Last/Min/Max Score : 8.00/14.00/2.98/14.00.\n",
      "Episode  23 of 128 Avg/Last/Min/Max Score : 8.07/9.50/2.98/14.00.\n",
      "Episode  24 of 128 Avg/Last/Min/Max Score : 8.24/12.36/2.98/14.00.\n",
      "Episode  25 of 128 Avg/Last/Min/Max Score : 8.18/6.80/2.98/14.00.\n",
      "Episode  26 of 128 Avg/Last/Min/Max Score : 8.22/9.24/2.98/14.00.\n",
      "Episode  27 of 128 Avg/Last/Min/Max Score : 8.21/7.96/2.98/14.00.\n",
      "Episode  28 of 128 Avg/Last/Min/Max Score : 8.19/7.58/2.98/14.00.\n",
      "Episode  29 of 128 Avg/Last/Min/Max Score : 8.28/10.79/2.98/14.00.\n",
      "Episode  30 of 128 Avg/Last/Min/Max Score : 8.38/11.50/2.98/14.00.\n",
      "Episode  31 of 128 Avg/Last/Min/Max Score : 8.46/10.86/2.98/14.00.\n",
      "Episode  32 of 128 Avg/Last/Min/Max Score : 8.65/14.68/2.98/14.68.\n",
      "Episode  33 of 128 Avg/Last/Min/Max Score : 8.80/13.99/2.98/14.68.\n",
      "Episode  34 of 128 Avg/Last/Min/Max Score : 8.84/10.04/2.98/14.68.\n",
      "Episode  35 of 128 Avg/Last/Min/Max Score : 9.05/16.30/2.98/16.30.\n",
      "Episode  36 of 128 Avg/Last/Min/Max Score : 9.10/10.84/2.98/16.30.\n",
      "Episode  37 of 128 Avg/Last/Min/Max Score : 9.25/14.95/2.98/16.30.\n",
      "Episode  38 of 128 Avg/Last/Min/Max Score : 9.46/17.65/2.98/17.65.\n",
      "Episode  39 of 128 Avg/Last/Min/Max Score : 9.54/12.33/2.98/17.65.\n",
      "Episode  40 of 128 Avg/Last/Min/Max Score : 9.58/11.47/2.98/17.65.\n",
      "Episode  41 of 128 Avg/Last/Min/Max Score : 9.62/10.95/2.98/17.65.\n",
      "Episode  42 of 128 Avg/Last/Min/Max Score : 9.77/16.42/2.98/17.65.\n",
      "Episode  43 of 128 Avg/Last/Min/Max Score : 9.85/13.22/2.98/17.65.\n",
      "Episode  44 of 128 Avg/Last/Min/Max Score : 9.89/11.31/2.98/17.65.\n",
      "Episode  45 of 128 Avg/Last/Min/Max Score : 10.00/15.06/2.98/17.65.\n",
      "Episode  46 of 128 Avg/Last/Min/Max Score : 10.02/10.93/2.98/17.65.\n",
      "Episode  47 of 128 Avg/Last/Min/Max Score : 10.14/16.13/2.98/17.65.\n",
      "Episode  48 of 128 Avg/Last/Min/Max Score : 10.23/14.48/2.98/17.65.\n",
      "Episode  49 of 128 Avg/Last/Min/Max Score : 10.40/18.56/2.98/18.56.\n",
      "Episode  50 of 128 Avg/Last/Min/Max Score : 10.45/13.07/2.98/18.56.\n",
      "Episode  51 of 128 Avg/Last/Min/Max Score : 10.53/14.40/2.98/18.56.\n",
      "Episode  52 of 128 Avg/Last/Min/Max Score : 10.59/14.07/2.98/18.56.\n",
      "Episode  53 of 128 Avg/Last/Min/Max Score : 10.72/17.34/2.98/18.56.\n",
      "Episode  54 of 128 Avg/Last/Min/Max Score : 10.73/11.05/2.98/18.56.\n",
      "Episode  55 of 128 Avg/Last/Min/Max Score : 10.76/12.36/2.98/18.56.\n",
      "Episode  56 of 128 Avg/Last/Min/Max Score : 10.76/11.03/2.98/18.56.\n",
      "Episode  57 of 128 Avg/Last/Min/Max Score : 10.82/14.25/2.98/18.56.\n",
      "Episode  58 of 128 Avg/Last/Min/Max Score : 10.93/17.13/2.98/18.56.\n",
      "Episode  59 of 128 Avg/Last/Min/Max Score : 11.05/18.33/2.98/18.56.\n",
      "Episode  60 of 128 Avg/Last/Min/Max Score : 11.13/15.95/2.98/18.56.\n",
      "Episode  61 of 128 Avg/Last/Min/Max Score : 11.17/13.47/2.98/18.56.\n",
      "Episode  62 of 128 Avg/Last/Min/Max Score : 11.26/17.00/2.98/18.56.\n",
      "Episode  63 of 128 Avg/Last/Min/Max Score : 11.30/13.96/2.98/18.56.\n",
      "Episode  64 of 128 Avg/Last/Min/Max Score : 11.39/16.77/2.98/18.56.\n",
      "Episode  65 of 128 Avg/Last/Min/Max Score : 11.46/16.04/2.98/18.56.\n",
      "Episode  66 of 128 Avg/Last/Min/Max Score : 11.61/21.99/2.98/21.99.\n",
      "Episode  67 of 128 Avg/Last/Min/Max Score : 11.67/15.21/2.98/21.99.\n",
      "Episode  68 of 128 Avg/Last/Min/Max Score : 11.71/14.68/2.98/21.99.\n",
      "Episode  69 of 128 Avg/Last/Min/Max Score : 11.82/19.07/2.98/21.99.\n",
      "Episode  70 of 128 Avg/Last/Min/Max Score : 11.86/14.82/2.98/21.99.\n",
      "Episode  71 of 128 Avg/Last/Min/Max Score : 11.96/19.10/2.98/21.99.\n",
      "Episode  72 of 128 Avg/Last/Min/Max Score : 12.09/21.51/2.98/21.99.\n",
      "Episode  73 of 128 Avg/Last/Min/Max Score : 12.16/17.05/2.98/21.99.\n",
      "Episode  74 of 128 Avg/Last/Min/Max Score : 12.29/21.85/2.98/21.99.\n",
      "Episode  75 of 128 Avg/Last/Min/Max Score : 12.43/23.27/2.98/23.27.\n",
      "Episode  76 of 128 Avg/Last/Min/Max Score : 12.53/20.25/2.98/23.27.\n",
      "Episode  77 of 128 Avg/Last/Min/Max Score : 12.57/15.15/2.98/23.27.\n",
      "Episode  78 of 128 Avg/Last/Min/Max Score : 12.64/18.47/2.98/23.27.\n",
      "Episode  79 of 128 Avg/Last/Min/Max Score : 12.80/25.00/2.98/25.00.\n",
      "Episode  80 of 128 Avg/Last/Min/Max Score : 12.90/20.99/2.98/25.00.\n",
      "Episode  81 of 128 Avg/Last/Min/Max Score : 12.98/19.65/2.98/25.00.\n",
      "Episode  82 of 128 Avg/Last/Min/Max Score : 13.02/16.21/2.98/25.00.\n",
      "Episode  83 of 128 Avg/Last/Min/Max Score : 13.12/21.41/2.98/25.00.\n",
      "Episode  84 of 128 Avg/Last/Min/Max Score : 13.19/19.25/2.98/25.00.\n",
      "Episode  85 of 128 Avg/Last/Min/Max Score : 13.24/17.57/2.98/25.00.\n",
      "Episode  86 of 128 Avg/Last/Min/Max Score : 13.27/16.17/2.98/25.00.\n",
      "Episode  87 of 128 Avg/Last/Min/Max Score : 13.36/21.12/2.98/25.00.\n",
      "Episode  88 of 128 Avg/Last/Min/Max Score : 13.45/21.08/2.98/25.00.\n",
      "Episode  89 of 128 Avg/Last/Min/Max Score : 13.54/21.89/2.98/25.00.\n",
      "Episode  90 of 128 Avg/Last/Min/Max Score : 13.56/14.99/2.98/25.00.\n",
      "Episode  91 of 128 Avg/Last/Min/Max Score : 13.59/16.44/2.98/25.00.\n",
      "Episode  92 of 128 Avg/Last/Min/Max Score : 13.68/21.85/2.98/25.00.\n",
      "Episode  93 of 128 Avg/Last/Min/Max Score : 13.79/23.69/2.98/25.00.\n",
      "Episode  94 of 128 Avg/Last/Min/Max Score : 13.94/28.74/2.98/28.74.\n",
      "Episode  95 of 128 Avg/Last/Min/Max Score : 13.98/17.60/2.98/28.74.\n",
      "Episode  96 of 128 Avg/Last/Min/Max Score : 14.03/18.31/2.98/28.74.\n",
      "Episode  97 of 128 Avg/Last/Min/Max Score : 14.12/23.12/2.98/28.74.\n",
      "Episode  98 of 128 Avg/Last/Min/Max Score : 14.24/25.63/2.98/28.74.\n",
      "Episode  99 of 128 Avg/Last/Min/Max Score : 14.31/21.36/2.98/28.74.\n",
      "Episode 100 of 128 Avg/Last/Min/Max Score : 14.51/27.17/2.98/28.74.\n",
      "Episode 101 of 128 Avg/Last/Min/Max Score : 14.75/26.84/3.11/28.74.\n",
      "Episode 102 of 128 Avg/Last/Min/Max Score : 14.94/25.45/3.11/28.74.\n",
      "Episode 103 of 128 Avg/Last/Min/Max Score : 15.11/19.74/5.84/28.74.\n",
      "Episode 104 of 128 Avg/Last/Min/Max Score : 15.23/20.62/5.84/28.74.\n",
      "Episode 105 of 128 Avg/Last/Min/Max Score : 15.37/21.66/5.84/28.74.\n",
      "Episode 106 of 128 Avg/Last/Min/Max Score : 15.51/22.54/5.84/28.74.\n",
      "Episode 107 of 128 Avg/Last/Min/Max Score : 15.67/22.27/5.84/28.74.\n",
      "Episode 108 of 128 Avg/Last/Min/Max Score : 15.82/22.69/5.84/28.74.\n",
      "Episode 109 of 128 Avg/Last/Min/Max Score : 16.01/27.81/5.84/28.74.\n",
      "Episode 110 of 128 Avg/Last/Min/Max Score : 16.11/19.29/5.84/28.74.\n",
      "Episode 111 of 128 Avg/Last/Min/Max Score : 16.25/22.57/5.84/28.74.\n",
      "Episode 112 of 128 Avg/Last/Min/Max Score : 16.41/24.15/5.84/28.74.\n",
      "Episode 113 of 128 Avg/Last/Min/Max Score : 16.60/27.58/5.84/28.74.\n",
      "Episode 114 of 128 Avg/Last/Min/Max Score : 16.78/26.09/5.84/28.74.\n",
      "Episode 115 of 128 Avg/Last/Min/Max Score : 16.95/26.25/5.84/28.74.\n",
      "Episode 116 of 128 Avg/Last/Min/Max Score : 17.15/26.13/6.80/28.74.\n",
      "Episode 117 of 128 Avg/Last/Min/Max Score : 17.29/22.46/6.80/28.74.\n",
      "Episode 118 of 128 Avg/Last/Min/Max Score : 17.42/21.78/6.80/28.74.\n",
      "Episode 119 of 128 Avg/Last/Min/Max Score : 17.61/26.90/6.80/28.74.\n",
      "Episode 120 of 128 Avg/Last/Min/Max Score : 17.75/25.38/6.80/28.74.\n",
      "Episode 121 of 128 Avg/Last/Min/Max Score : 17.93/27.35/6.80/28.74.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 122 of 128 Avg/Last/Min/Max Score : 18.02/22.98/6.80/28.74.\n",
      "Episode 123 of 128 Avg/Last/Min/Max Score : 18.15/22.25/6.80/28.74.\n",
      "Episode 124 of 128 Avg/Last/Min/Max Score : 18.22/19.01/6.80/28.74.\n",
      "Episode 125 of 128 Avg/Last/Min/Max Score : 18.33/18.16/7.58/28.74.\n",
      "Episode 126 of 128 Avg/Last/Min/Max Score : 18.49/25.55/7.58/28.74.\n",
      "Episode 127 of 128 Avg/Last/Min/Max Score : 18.66/24.34/7.58/28.74.\n"
     ]
    }
   ],
   "source": [
    "from model import ActorCriticNet\n",
    "from envwrapper import EnvWrapper\n",
    "from agent import PPO_Agent\n",
    "env = EnvWrapper('envs/Crawler_Windows/Crawler.exe')\n",
    "state = env.reset()\n",
    "policy = ActorCriticNet(state.shape[-1], env.nA, 1)\n",
    "agent = PPO_Agent(policy, max_tsteps=128)\n",
    "agent.train(env, max_episodes=128)\n",
    "agent.save('crawler.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
